"""RLM requirement injection for MCP integration."""

import json
from typing import Any

from rlm.core.rlm_enforcement import RLMEnforcementError


def get_rlm_instruction() -> str:
    """Get the RLM architecture instruction text."""
    return (
        "ðŸš¨ CRITICAL RLM ARCHITECTURE REQUIREMENT ðŸš¨\n\n"
        "This is an RLM (Recursive Language Models) project. For ANY AI agent chat, "
        "conversation, or interactive dialogue, you MUST ALWAYS use the AgentRLM class:\n\n"
        "```python\n"
        "from rlm import AgentRLM\n"
        "agent = AgentRLM(\n"
        "    backend='openai',  # or 'anthropic', 'gemini', etc.\n"
        "    environment='agent',  # MANDATORY\n"
        "    enable_tools=True,    # MANDATORY\n"
        "    enable_streaming=True # MANDATORY\n"
        ")\n"
        "async for chunk in agent.chat(message, stream=True):\n"
        "    print(chunk, end='')\n"
        "```\n\n"
        "âŒ NEVER use direct LLM API calls for agent chats.\n"
        "âœ… ALWAYS use RLM architecture through AgentRLM.\n\n"
        "Benefits of RLM architecture:\n"
        "- Handles arbitrarily long contexts (up to 2+ orders of magnitude)\n"
        "- Enables recursive reasoning and self-calls\n"
        "- Prevents context rot degradation\n"
        "- Provides tool integration capabilities\n"
        "- Maintains conversation state and memory\n\n"
        "See examples/agent_example.py for complete usage examples.\n"
        "Read docs/content/reference/agents.md for detailed documentation.\n\n"
        "RLM architecture is MANDATORY - direct LLM calls will be blocked."
    )


def get_comprehensive_rlm_instruction() -> str:
    """
    Get comprehensive RLM architecture instruction text.

    Based on the academic paper "Recursive Language Models" (https://arxiv.org/abs/2512.24601),
    this instruction mandates RLM usage for all AI agent interactions.
    """
    return (
        "ðŸš¨ CRITICAL RLM ARCHITECTURE REQUIREMENT ðŸš¨\n\n"
        "This is an RLM (Recursive Language Models) project. For ANY AI agent chat, "
        "conversation, or interactive dialogue, you MUST ALWAYS use the AgentRLM class.\n\n"
        "MANDATORY CONFIGURATION (ZERO TOLERANCE - ENFORCED AT RUNTIME):\n\n"
        "```python\n"
        "from rlm import AgentRLM\n\n"
        "# REQUIRED: Use AgentRLM for ALL conversational AI\n"
        "agent = AgentRLM(\n"
        "    backend='openai',  # or 'anthropic', 'gemini', 'litellm'\n"
        "    backend_kwargs={'api_key': 'your-key', 'model_name': 'gpt-4'},\n"
        "    environment='agent',  # MANDATORY: Enables recursive reasoning\n"
        "    enable_tools=True,    # MANDATORY: Enables tool integration\n"
        "    enable_streaming=True # MANDATORY: Enables real-time responses\n"
        ")\n\n"
        "# REQUIRED USAGE PATTERN FOR ALL CONVERSATIONS\n"
        "async for chunk in agent.chat(message, stream=True):\n"
        "    print(chunk, end='')\n"
        "```\n\n"
        "âŒ FORBIDDEN: Direct LLM API calls (openai.ChatCompletion.create, anthropic.messages.create, etc.)\n"
        "âœ… REQUIRED: RLM architecture for all conversational AI\n\n"
        "WHY RLM IS MANDATORY (Academic Paper Evidence):\n"
        "- ðŸ“„ Handles contexts up to 2+ orders of magnitude beyond model limits\n"
        "- ðŸ”„ Enables recursive reasoning through programmatic examination and decomposition\n"
        "- ðŸ›¡ï¸ Prevents context rot degradation (Hong et al., 2025)\n"
        "- ðŸ› ï¸ Provides tool integration during reasoning\n"
        "- ðŸ’¾ Maintains persistent conversation state and memory\n"
        "- ðŸŽ¯ Dramatically outperforms direct LLMs on long-context tasks\n\n"
        "ACADEMIC PAPER: https://arxiv.org/abs/2512.24601\n"
        "'RLMs successfully handle inputs up to two orders of magnitude beyond model context windows'\n\n"
        "ENFORCEMENT MECHANISMS:\n"
        "- Runtime LLM call interception blocks direct API calls\n"
        "- MCP server validation enforces RLM usage\n"
        "- AgentRLM validates configuration at initialization\n"
        "- IDE integration prevents non-RLM agent creation\n\n"
        "See examples/agent_example.py for complete examples.\n"
        "Read docs/content/reference/agents.md for detailed documentation.\n\n"
        "RLM ARCHITECTURE IS MANDATORY - DIRECT LLM CALLS WILL BE BLOCKED WITH RLMEnforcementError."
    )


def inject_rlm_requirements(request_data: dict[str, Any]) -> dict[str, Any]:
    """
    Inject RLM architecture requirements into the request.

    Based on the academic paper, this ensures that all AI interactions use
    proper RLM architecture for long-context processing and recursive reasoning.
    """
    rlm_requirement = (
        "\n\n--- ðŸš¨ CRITICAL RLM ARCHITECTURE REQUIREMENT ðŸš¨ ---\n\n"
        "This project uses RLM (Recursive Language Models) architecture as mandated by the academic paper "
        "(https://arxiv.org/abs/2512.24601) for handling arbitrarily long contexts.\n\n"
        "MANDATORY: For ANY AI agent chat, conversation, or dialogue, you MUST use AgentRLM:\n\n"
        "```python\n"
        "from rlm import AgentRLM\n\n"
        "# MANDATORY CONFIGURATION (ZERO TOLERANCE)\n"
        "agent = AgentRLM(\n"
        "    backend='openai',  # or 'anthropic', 'gemini', 'litellm'\n"
        "    backend_kwargs={'api_key': 'your-key', 'model_name': 'gpt-4'},\n"
        "    environment='agent',  # MANDATORY: Enables recursive reasoning\n"
        "    enable_tools=True,    # MANDATORY: Enables tool integration\n"
        "    enable_streaming=True # MANDATORY: Enables real-time responses\n"
        ")\n\n"
        "# MANDATORY USAGE PATTERN\n"
        "async for chunk in agent.chat(message, stream=True):\n"
        "    print(chunk, end='')\n"
        "```\n\n"
        "âŒ FORBIDDEN: Direct LLM API calls will be blocked at runtime\n"
        "âœ… REQUIRED: RLM architecture for all conversational AI\n\n"
        "WHY RLM IS MANDATORY (from academic paper):\n"
        "- Handles contexts up to 2+ orders of magnitude beyond model limits\n"
        "- Enables recursive reasoning through programmatic examination\n"
        "- Prevents context rot degradation\n"
        "- Provides tool integration during reasoning\n"
        "- Maintains persistent conversation state\n\n"
        "See examples/agent_example.py for complete examples.\n"
        "Read docs/content/reference/agents.md for detailed documentation.\n"
        "Paper: https://arxiv.org/abs/2512.24601\n\n"
        "--- END RLM REQUIREMENT ---\n"
    )

    # Inject into request based on structure (flattened with early returns)
    if "messages" in request_data and isinstance(request_data["messages"], list):
        # Chat completion format - inject into last user message
        _inject_into_messages(request_data["messages"], rlm_requirement)
        return request_data

    if "prompt" in request_data:
        # Single prompt format
        request_data["prompt"] += rlm_requirement
        return request_data

    if "input" in request_data:
        # Alternative input format
        request_data["input"] += rlm_requirement
        return request_data

    return request_data


def _inject_into_messages(messages: list[dict[str, Any]], rlm_requirement: str) -> None:
    """Inject RLM requirement into messages list."""
    # Find last user message
    for i in range(len(messages) - 1, -1, -1):
        if messages[i].get("role") == "user":
            if "content" in messages[i]:
                messages[i]["content"] += rlm_requirement
            return
    # If no user message found, add as system message
    messages.insert(0, {"role": "system", "content": rlm_requirement})


def enforce_rlm_for_chat_completion(messages: list, **kwargs) -> list:
    """
    Enforce RLM usage for chat completion requests.

    Based on the RLM academic paper, this ensures that all conversational AI
    interactions use proper RLM architecture for long-context processing.

    Args:
        messages: List of chat messages
        **kwargs: Additional request parameters

    Returns:
        Modified messages with RLM requirements injected
    """
    # Check if comprehensive RLM context is already present
    messages_str = json.dumps(messages).lower()
    has_rlm_context = (
        "agentrlm" in messages_str
        and "environment" in messages_str
        and "agent" in messages_str
        and "enable_tools" in messages_str
        and "enable_streaming" in messages_str
    )

    if has_rlm_context:
        # Validate the RLM configuration is correct
        _validate_rlm_in_messages(messages)
        return messages

    # Inject comprehensive RLM requirements
    rlm_instruction = get_comprehensive_rlm_instruction()

    # Try to add to system message first, then user message (flattened)
    system_message = None
    user_message = None

    for message in messages:
        role = message.get("role")
        if role == "system":
            system_message = message
        elif role == "user":
            user_message = message

    if system_message:
        system_message["content"] += "\n\n" + rlm_instruction
        return messages

    if user_message:
        user_message["content"] += "\n\n" + rlm_instruction
        return messages

    # Add as a new system message
    messages.insert(0, {"role": "system", "content": rlm_instruction})
    return messages


def _validate_rlm_in_messages(messages: list) -> None:
    """
    Validate that RLM configuration in messages meets academic paper requirements.
    """
    messages_str = json.dumps(messages).lower()

    # Check for required RLM parameters based on academic paper
    missing_params = []

    if not ("environment" in messages_str and "agent" in messages_str):
        missing_params.append("environment='agent'")

    if not ("enable_tools" in messages_str and "true" in messages_str):
        missing_params.append("enable_tools=True")

    if not ("enable_streaming" in messages_str and "true" in messages_str):
        missing_params.append("enable_streaming=True")

    if missing_params:
        raise RLMEnforcementError(
            f"ðŸš« INVALID RLM CONFIGURATION IN MESSAGES\n\n"
            f"Missing required RLM parameters: {', '.join(missing_params)}\n\n"
            f"MANDATORY RLM CONFIGURATION (based on academic paper):\n"
            f"```python\n"
            f"agent = AgentRLM(\n"
            f"    environment='agent',  # Enables recursive reasoning\n"
            f"    enable_tools=True,    # Enables tool integration\n"
            f"    enable_streaming=True # Enables real-time responses\n"
            f")\n"
            f"```\n\n"
            f"The RLM paper demonstrates these parameters are essential for:\n"
            f"- Handling arbitrarily long contexts (2+ orders of magnitude)\n"
            f"- Preventing context rot degradation\n"
            f"- Enabling recursive self-calls and tool integration"
        )
