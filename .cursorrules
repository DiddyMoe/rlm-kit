# RLM (Recursive Language Models) — Cursor Rules

## Overview

This repository contains the **RLM inference engine** and its **VS Code/Cursor extension**.
RLM is a task-agnostic paradigm for language models to handle near-infinite length
contexts via programmatic examination, decomposition, and recursive self-calls.

**Paper:** [Recursive Language Models](https://arxiv.org/abs/2512.24601)

## Cursor Integration

Cursor integrates with RLM via the **MCP gateway** (`scripts/rlm_mcp_gateway.py`).
The gateway exposes RLM tools that Cursor's agent can invoke directly.

### How to use RLM in Cursor

1. The MCP gateway is configured in `.cursor/mcp.json`
2. Available tools include: session management, filesystem access, code execution, LLM completions
3. Use `rlm.complete` for full RLM reasoning over large contexts
4. Use `rlm.exec.run` for direct Python code execution

### Key MCP Tools

| Tool | Purpose |
|------|---------|
| `rlm.session.create` | Start a new RLM session |
| `rlm.complete` | Run a full RLM completion (recursive reasoning) |
| `rlm.exec.run` | Execute Python code in the REPL |
| `rlm.fs.list` | List files in the workspace |
| `rlm.search.query` | Semantic search across files |
| `rlm.search.regex` | Regex search across files |

## Architecture

| Layer | Location | Purpose |
|-------|----------|---------|
| Core | `rlm/core/` | RLM loop, LM handler, iteration, comms, types |
| Clients | `rlm/clients/` | LM API integrations (OpenAI, Anthropic, Gemini, etc.) |
| Environments | `rlm/environments/` | REPL execution: Local, Docker, Modal, Prime, Daytona |
| MCP Gateway | `rlm/mcp_gateway/` | MCP server for IDE integration |
| Extension | `vscode-extension/` | VS Code Chat Participant + Python backend bridge |

## Code Style

- **Python:** `ruff` for lint/format, explicit types, `snake_case` methods, `PascalCase` classes
- **TypeScript:** Strict mode, no unused variables, no implicit returns
- **Errors:** Fail fast and loud, no silent fallbacks
- **Testing:** `pytest` for Python, all tests must pass before PR

## Key Patterns

- **LM Client:** Inherit `BaseLM`, implement `completion`, `acompletion`, `get_usage_summary`, `get_last_usage`
- **Environment:** Inherit `NonIsolatedEnv`/`IsolatedEnv`, implement `setup`, `load_context`, `execute_code`, `cleanup`
- **RLM Loop:** `RLM.completion(prompt)` → iterative REPL execution → `FINAL(answer)` or `FINAL_VAR(var)`
- **Sub-LLM calls:** `llm_query(prompt)` / `llm_query_batched(prompts)` inside REPL code
